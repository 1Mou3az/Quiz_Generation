{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac09373e",
      "metadata": {
        "id": "ac09373e",
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# for training\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install fuzzywuzzy python-Levenshtein\n",
        "!pip install bitsandbytes==0.41.3\n",
        "!pip install -q -U sentencepiece\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U datasets\n",
        "!pip install nltk==3.5.0\n",
        "!pip install bert-score\n",
        "!pip install rouge\n",
        "#for inference time\n",
        "!pip install PyPDF2\n",
        "!pip install keybert\n",
        "!pip install keybert[use]\n",
        "!pip install keybert[spacy]\n",
        "!pip install keybert[flair]\n",
        "!pip install keybert[gensim]\n",
        "!pip install sense2vec==2.0.1\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install git+https://github.com/boudinfl/pke.git\n",
        "!wget https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\n",
        "!tar -xvf  s2v_reddit_2015_md.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c06f6c13",
      "metadata": {
        "id": "c06f6c13",
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "if torch.cuda.is_available()==True :\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SXqhksrXRLDW",
      "metadata": {
        "id": "SXqhksrXRLDW"
      },
      "source": [
        "# #########################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pYfKMpAjRLDr",
      "metadata": {
        "id": "pYfKMpAjRLDr"
      },
      "source": [
        "# For Inference Time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b3f1d57",
      "metadata": {
        "id": "7b3f1d57",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "### Cleaning the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a09e9eea",
      "metadata": {
        "id": "a09e9eea",
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn\n",
        "stop_words = stopwords.words('english')\n",
        "# arabic_stopwords = stopwords.words('arabic')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb1c452e",
      "metadata": {
        "id": "cb1c452e",
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def remove_non_ascii(text):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    return text.encode('ascii','ignore').decode()\n",
        "    #return ''.join(char for char in text if char.isalpha() and char.isnumeric() or 'ARABIC' in unicodedata.name(char, ''))\n",
        "\n",
        "def remove_brackets_num(text):\n",
        "    return re.sub(\"\\*?\",\"\",text)\n",
        "\n",
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "def replace_numbers(text):\n",
        "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
        "    return re.sub(r'\\d+','',text)\n",
        "\n",
        "def remove_whitespace(text):\n",
        "      return text.strip()\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    punctuation= '''!()[]{};:'\"\\<>/?$%^&*_`~='''\n",
        "    for punc in punctuation:\n",
        "        text=text.replace(punc,\"\")\n",
        "    return text\n",
        "\n",
        "def remove_emails(text):\n",
        "    return re.sub(r'[A-Za-z0-9]*@[A-Za-z]*\\.?[A-Za-z0-9]*', \"\", text)\n",
        "\n",
        "def text2words(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "def remove_stopwords(words,stop_words):\n",
        "    return [word for word in words if word not in stop_words]\n",
        "\n",
        "\n",
        "def normalize_text(text):\n",
        "    # text = remove_non_ascii(text)\n",
        "    text= remove_brackets_num(text)\n",
        "    text = to_lowercase(text)\n",
        "    #text=replace_numbers(text)\n",
        "    text= remove_whitespace(text)\n",
        "    text = remove_punctuation(text)\n",
        "    text= remove_emails(text)\n",
        "    words = text2words(text)\n",
        "    #words = remove_stopwords(words, stop_words)\n",
        "\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pMoY6e33RLD1",
      "metadata": {
        "id": "pMoY6e33RLD1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "N_text=normalize_text(text)\n",
        "N_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d83352b",
      "metadata": {
        "id": "6d83352b",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "### Generate KeyWords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c066f60",
      "metadata": {
        "id": "4c066f60",
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from keybert import KeyBERT\n",
        "kw_tool = KeyBERT()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7a1388c",
      "metadata": {
        "id": "c7a1388c",
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# def jaccard_similarity(word1, word2):\n",
        "#     set1 = set(word1)\n",
        "#     set2 = set(word2)\n",
        "\n",
        "#     intersection = len(set1.intersection(set2))\n",
        "#     union = len(set1.union(set2))\n",
        "\n",
        "#     similarity = intersection / union if union > 0 else 0\n",
        "#     return similarity\n",
        "\n",
        "def calculate_similarity(sentence1, sentence2):\n",
        "    # Initialize Porter Stemmer\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Tokenize and stem the sentences\n",
        "    stemmed_sentence1 = ' '.join([stemmer.stem(word) for word in sentence1.split()])\n",
        "    stemmed_sentence2 = ' '.join([stemmer.stem(word) for word in sentence2.split()])\n",
        "\n",
        "    # Convert the stemmed sentences into vectors\n",
        "    vectorizer = CountVectorizer().fit([stemmed_sentence1, stemmed_sentence2])\n",
        "    vectorized_sentences = vectorizer.transform([stemmed_sentence1, stemmed_sentence2])\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    cosine_sim = cosine_similarity(vectorized_sentences)[0][1]\n",
        "\n",
        "    return cosine_sim\n",
        "\n",
        "\n",
        "def extract_keywords_from_text1(text):\n",
        "    # Extract keywords from the given text\n",
        "    KeyBERT1 = kw_tool.extract_keywords(text, keyphrase_ngram_range=(1,1), top_n=10)\n",
        "    KeyBERT2 = kw_tool.extract_keywords(text, keyphrase_ngram_range=(2,2), top_n=10)\n",
        "\n",
        "    # Combine all extracted keywords\n",
        "    all_keywords = [key[0] for key in KeyBERT1] + \\\n",
        "                   [key[0] for key in KeyBERT2]\n",
        "    # Filter out empty keywords\n",
        "    all_keywords = [keyword for keyword in all_keywords if keyword]\n",
        "\n",
        "    # Filter out very similar keywords\n",
        "    similarity_threshold_between_keywords = 0.4  # Threshold for similarity between keywords\n",
        "    unique_keywords = []\n",
        "    for keyword in all_keywords:\n",
        "        if all(calculate_similarity(keyword, existing_keyword) < similarity_threshold_between_keywords for existing_keyword in unique_keywords):\n",
        "            unique_keywords.append(keyword)\n",
        "\n",
        "    return unique_keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AZfAieS3RLD5",
      "metadata": {
        "id": "AZfAieS3RLD5"
      },
      "outputs": [],
      "source": [
        "unique_keywords1=extract_keywords_from_text1(N_text)\n",
        "unique_keywords1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3jrKiUFeRLD6",
      "metadata": {
        "id": "3jrKiUFeRLD6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pke\n",
        "\n",
        "def extract_keywords_from_text2(text):\n",
        "    # Initialize keyphrase extraction model, here TopicRank\n",
        "    extractor = pke.unsupervised.TopicRank()\n",
        "\n",
        "    # Load the content of the document\n",
        "    extractor.load_document(input=text, language='en')\n",
        "\n",
        "    # Keyphrase candidate selection: in the case of TopicRank: sequences of nouns\n",
        "    # and adjectives (i.e., `(Noun|Adj)*`)\n",
        "    extractor.candidate_selection()\n",
        "\n",
        "    # Candidate weighting: using a random walk algorithm\n",
        "    extractor.candidate_weighting()\n",
        "\n",
        "    # N-best selection, keyphrases contains the 10 highest scored candidates\n",
        "    keyphrases = extractor.get_n_best(n=20)\n",
        "\n",
        "    # Extract keyphrases\n",
        "    keywords = [keyphrase for keyphrase, score in keyphrases]\n",
        "\n",
        "    # Calculate similarity with keywords\n",
        "    unique_keywords = []\n",
        "\n",
        "    # Handling unigrams and bigrams separately\n",
        "    unigrams = [keyphrase for keyphrase in keywords if len(keyphrase.split()) == 1]\n",
        "    bigrams = [keyphrase for keyphrase in keywords if len(keyphrase.split()) == 2]\n",
        "\n",
        "    # Add unigrams to unique_keywords directly\n",
        "    unique_keywords.extend(unigrams)\n",
        "\n",
        "    # Filter bigrams based on similarity with existing keywords\n",
        "    for keyphrase in bigrams:\n",
        "        similarity = calculate_similarity(keyphrase, ' '.join(keywords))\n",
        "        if similarity < 0.4:  # Adjust the similarity threshold as needed\n",
        "            unique_keywords.append(keyphrase)\n",
        "\n",
        "    return unique_keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-9vXtsYtRLD7",
      "metadata": {
        "id": "-9vXtsYtRLD7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "unique_keywords2=extract_keywords_from_text2(N_text)\n",
        "unique_keywords2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05818b09",
      "metadata": {
        "id": "05818b09",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "### Generate Distractors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86fe680f",
      "metadata": {
        "id": "86fe680f",
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sense2vec import Sense2Vec\n",
        "# load sense2vec vectors\n",
        "s2v = Sense2Vec().from_disk('s2v_old')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3T9Pwa9eRLD9",
      "metadata": {
        "id": "3T9Pwa9eRLD9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "def sense2vec_get_words(word, s2v):\n",
        "    output = []\n",
        "    word = word.lower()\n",
        "\n",
        "    sense = s2v.get_best_sense(word)\n",
        "    similarity_threshold = 0.3\n",
        "    out = []\n",
        "\n",
        "    if sense is not None:\n",
        "        most_similar = s2v.most_similar(sense, n=20)\n",
        "        for sim in most_similar:\n",
        "            append_word= sim[0].split(\"|\")[0].replace(\"_\", \" \").lower()\n",
        "\n",
        "            # Check similarity with keyword\n",
        "            similarity_keyword = calculate_similarity(word, append_word)\n",
        "            #print(f\"Similarity between '{word}' and '{append_word}': {similarity_keyword}\")\n",
        "\n",
        "            # Check if similarity with keyword is above the threshold\n",
        "            if similarity_keyword >= similarity_threshold:\n",
        "                continue\n",
        "\n",
        "            # Check similarity with existing distractors\n",
        "            similarity_to_existing = [calculate_similarity(append_word, existing_distractor) for existing_distractor in output]\n",
        "\n",
        "            # Check if similarity with any existing distractor is above the threshold\n",
        "            if any(similarity >= similarity_threshold for similarity in similarity_to_existing):\n",
        "                continue\n",
        "\n",
        "            # If the conditions are met, append the word to the list of output\n",
        "            output.append(append_word.title())\n",
        "\n",
        "        out = list(OrderedDict.fromkeys(output))\n",
        "    return out[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qIBD-g_0RLD_",
      "metadata": {
        "id": "qIBD-g_0RLD_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "for word in unique_keywords:\n",
        "    existing_distractor=sense2vec_get_words(word, s2v)\n",
        "    print(word , existing_distractor)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z8pxtjdmRLEA",
      "metadata": {
        "id": "z8pxtjdmRLEA"
      },
      "source": [
        "### At Inference Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7GrEOlC7RLEA",
      "metadata": {
        "id": "7GrEOlC7RLEA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#general question generation model\n",
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "HUGGING_FACE_USER_NAME='mou3az'\n",
        "model_name='QuestionGeneration'\n",
        "peft_model_id = f\"{HUGGING_FACE_USER_NAME}/{model_name}\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=False, device_map='auto')\n",
        "G_tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the Lora model\n",
        "G_model = PeftModel.from_pretrained(model, peft_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QQbt6nc6RLEB",
      "metadata": {
        "id": "QQbt6nc6RLEB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def generate_questions(context, answer, distractors):\n",
        "    device = next(G_model.parameters()).device\n",
        "    input_text = f\"Given the context '{context}' and the answer '{answer}' , what question can be asked?\"\n",
        "    encoding = G_tokenizer.encode_plus(input_text, padding=True,truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    output_tokens = G_model.generate(**encoding, early_stopping=True, num_beams=5, num_return_sequences=1, no_repeat_ngram_size=2, max_length=200)\n",
        "    question = G_tokenizer.decode(output_tokens[0], skip_special_tokens=True).replace(\"question:\", \"\").strip()\n",
        "    return question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4aFeTnPXRLED",
      "metadata": {
        "id": "4aFeTnPXRLED",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def process_and_generate_questions(example):\n",
        "    N_text_file, keywords, keyword_question_distractors = \"\", [], []\n",
        "\n",
        "    # Check if example is not None and not empty\n",
        "    if example is not None and example.strip():\n",
        "        # Remove empty lines between sentences\n",
        "        cleaned_text = '\\n'.join(line.strip() for line in example.split('\\n') if line.strip())\n",
        "\n",
        "        # Concatenate lines and separate them with a period\n",
        "        concatenated_text = '.'.join(cleaned_text.split('\\n'))\n",
        "\n",
        "        # Tokenize the text using G_tokenizer\n",
        "        tokens = G_tokenizer.tokenize(concatenated_text)\n",
        "\n",
        "        # Check if the number of tokens is less than 1024\n",
        "        if len(tokens) < 1024: # or 512 for google-flan-t5\n",
        "            # Normalize the concatenated text\n",
        "            N_text_file = normalize_text(concatenated_text)\n",
        "\n",
        "            # Check if N_text_file has text\n",
        "            if N_text_file:\n",
        "                keywords = extract_keywords_from_text(N_text_file)\n",
        "                if keywords:\n",
        "                    for word in keywords:\n",
        "                        current_distractors = sense2vec_get_words(word, s2v)\n",
        "                        if current_distractors:\n",
        "                            question = generate_questions(N_text_file, word)\n",
        "                            keyword_question_distractors.append((word, current_distractors, question))\n",
        "                else:\n",
        "                    print(\"No keywords generated.\")\n",
        "            else:\n",
        "                print(\"No text available.\")\n",
        "        else:\n",
        "            print(\"The tokenized text has more than 1024 tokens.\")\n",
        "    else:\n",
        "        print(\"The file is empty or large.\")\n",
        "        # Handle the case where the file is empty or cannot be loaded\n",
        "\n",
        "    result = [(N_text_file, keyword_question_distractors)]\n",
        "\n",
        "    return result\n",
        "\n",
        "# Example usage:\n",
        "txt= = \"\"\n",
        "Total_List = process_and_generate_questions(txt)\n",
        "\n",
        "# Print the context, keyword, question, and distractors\n",
        "for context, keyword_question_distractors in Total_List:\n",
        "    if context:\n",
        "        print(f\"context: {context}\")\n",
        "        print()\n",
        "        for keyword, distractors, question in keyword_question_distractors:\n",
        "            print(f\"Keyword: {keyword}\")\n",
        "            print(f\"Question: {question}\")\n",
        "            print(f\"Distractors: {distractors}\")\n",
        "            print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22BD4ZECRLEF",
      "metadata": {
        "id": "22BD4ZECRLEF"
      },
      "source": [
        "# ###########################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "exLV1HPzRLEG",
      "metadata": {
        "id": "exLV1HPzRLEG"
      },
      "source": [
        "# For Training Time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f30fd042",
      "metadata": {
        "id": "f30fd042",
        "tags": []
      },
      "source": [
        "### loading the model and the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c008ac5-d6ca-4569-a4d7-d6bf53503205",
      "metadata": {
        "id": "4c008ac5-d6ca-4569-a4d7-d6bf53503205",
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
        "\n",
        "model_name_or_path = \"facebook/bart-base\" # google/flan-t5-base\n",
        "\n",
        "# Define the quantization configuration with 4-bit\n",
        "# quantization_config = BitsAndBytesConfig(bit_width=4, bnb_4bit_compute_type=\"torch.float16\")\n",
        "\n",
        "# Load the model with the specified quantization configuration\n",
        "G_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    torch_dtype=torch.float32,\n",
        "    device_map='auto',\n",
        ")\n",
        "G_tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dba4b9f7",
      "metadata": {
        "id": "dba4b9f7",
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "Lora_config = LoraConfig(\n",
        "    r=18,\n",
        "    lora_alpha=12,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"], # or q and v for other models\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM\n",
        ")\n",
        "\n",
        "L_model = get_peft_model(G_model, Lora_config)\n",
        "print(L_model.print_trainable_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55751b0d",
      "metadata": {
        "id": "55751b0d",
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, load_dataset, concatenate_datasets\n",
        "\n",
        "# Load and preprocess General English data from SQuAD v2 for training set\n",
        "en_g_train = (\n",
        "    load_dataset(\"squad_v2\", split=\"train\")\n",
        "    .filter(lambda example: example[\"context\"] is not None\n",
        "            and example[\"question\"] is not None\n",
        "            and example[\"answers\"][\"text\"] is not None\n",
        "            and len(example[\"answers\"][\"text\"]) > 0\n",
        "            and len(example[\"context\"]) > 0\n",
        "            and len(example[\"question\"]) > 0\n",
        "            and len(example[\"context\"].split()) < 780\n",
        "            and len(example[\"question\"].split()) < 780)\n",
        "    .shuffle(seed=42)\n",
        "    .select(range(20000))\n",
        "    .map(lambda example: {\"context\": example[\"context\"], \"question\": example[\"question\"], \"answer\": example[\"answers\"][\"text\"][0]})\n",
        "    .remove_columns([\"id\", \"title\", \"answers\"])\n",
        ")\n",
        "\n",
        "# Load and preprocess General English data from SQuAD v2 for validation set\n",
        "en_g_validation = (\n",
        "    load_dataset(\"squad_v2\", split=\"validation\")\n",
        "    .filter(lambda example: example[\"context\"] is not None\n",
        "            and example[\"question\"] is not None\n",
        "            and example[\"answers\"][\"text\"] is not None\n",
        "            and len(example[\"answers\"][\"text\"]) > 0\n",
        "            and len(example[\"context\"]) > 0\n",
        "            and len(example[\"question\"]) > 0\n",
        "            and len(example[\"context\"].split()) < 780\n",
        "            and len(example[\"question\"].split()) < 780)\n",
        "    .shuffle(seed=42)\n",
        "    .select(range(1000))\n",
        "    .map(lambda example: {\"context\": example[\"context\"], \"question\": example[\"question\"], \"answer\": example[\"answers\"][\"text\"][0]})\n",
        "    .remove_columns([\"id\", \"title\", \"answers\"])\n",
        ")\n",
        "\n",
        "# Load and preprocess Different English data from DROPOUT\n",
        "en_d_train = (\n",
        "    load_dataset(\"drop\", split=\"train\")\n",
        "    .filter(lambda example: example[\"passage\"] is not None\n",
        "            and example[\"question\"] is not None\n",
        "            and example[\"answers_spans\"][\"spans\"] is not None\n",
        "            and len(example[\"answers_spans\"][\"spans\"]) > 0\n",
        "            and len(example[\"passage\"]) > 0\n",
        "            and len(example[\"question\"]) > 0\n",
        "            and len(example[\"passage\"].split()) < 780\n",
        "            and len(example[\"question\"].split()) < 780)\n",
        "    .shuffle(seed=42)\n",
        "    .select(range(20000))\n",
        "    .map(lambda example: {\"context\": example[\"passage\"], \"question\": example[\"question\"], \"answer\": example[\"answers_spans\"][\"spans\"][0]})\n",
        "    .remove_columns([\"section_id\", \"query_id\", \"answers_spans\", \"passage\"])\n",
        ")\n",
        "\n",
        "en_d_validation = (\n",
        "    load_dataset(\"drop\", split=\"validation\")\n",
        "    .filter(lambda example: example[\"passage\"] is not None\n",
        "            and example[\"question\"] is not None\n",
        "            and example[\"answers_spans\"][\"spans\"] is not None\n",
        "            and len(example[\"answers_spans\"][\"spans\"]) > 0\n",
        "            and len(example[\"passage\"]) > 0\n",
        "            and len(example[\"question\"]) > 0\n",
        "            and len(example[\"passage\"].split()) < 780\n",
        "            and len(example[\"question\"].split()) < 780)\n",
        "    .shuffle(seed=42)\n",
        "    .select(range(1000))\n",
        "    .map(lambda example: {\"context\": example[\"passage\"], \"question\": example[\"question\"], \"answer\": example[\"answers_spans\"][\"spans\"][0]})\n",
        "    .remove_columns([\"section_id\", \"query_id\", \"answers_spans\", \"passage\"])\n",
        ")\n",
        "\n",
        "# Load and preprocess additional English data\n",
        "en_additional = (\n",
        "    load_dataset(\"mou3az/Question-Answering-Generation-Choices\", split=\"train\")\n",
        "    .filter(lambda example: example[\"context\"] is not None\n",
        "            and len(example[\"context\"]) > 0\n",
        "            and len(example[\"context\"].split()) < 780\n",
        "            and example[\"question\"] is not None\n",
        "            and len(example[\"question\"]) > 0\n",
        "            and len(example[\"question\"].split()) < 780\n",
        "            and example.get(\"answer\") is not None\n",
        "            and len(example[\"answer\"]) > 0\n",
        "            and len(example[\"answer\"].split()) < 6)\n",
        "    .remove_columns([\"distractors\"])\n",
        ")\n",
        "\n",
        "# Split additional English data into training and validation sets\n",
        "en_additional_train = en_additional.select(range(20000))\n",
        "en_additional_validation = en_additional.select(range(20000, 21000))\n",
        "\n",
        "# Concatenate all datasets\n",
        "en_train = concatenate_datasets([en_g_train, en_d_train, en_additional_train])\n",
        "en_validation = concatenate_datasets([en_g_validation, en_d_validation, en_additional_validation])\n",
        "\n",
        "# Shuffle the datasets\n",
        "en_train = en_train.shuffle(seed=123)\n",
        "en_validation = en_validation.shuffle(seed=123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85d18ac3",
      "metadata": {
        "id": "85d18ac3",
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#For General data\n",
        "# import ast\n",
        "def create_prompt1(context, answer):\n",
        "    input_text = f\"Given the context '{context}' and the answer '{answer}' , what question can be asked?\"\n",
        "    return input_text\n",
        "\n",
        "def create_prompt2(question):\n",
        "    output_text = f\"question: {question}\"\n",
        "    return output_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c60d0004",
      "metadata": {
        "id": "c60d0004",
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#for english data\n",
        "en_train_data = en_train.map(lambda samples: G_tokenizer.encode_plus(create_prompt1(samples['context'], samples['answer']), padding=True), remove_columns=[\"context\", \"answer\", \"question\",\"distractors\"])\n",
        "en_validation_data = en_validation.map(lambda samples: G_tokenizer.encode_plus(create_prompt1(samples['context'], samples['answer']), padding=True), remove_columns=[\"context\", \"answer\", \"question\",'distractors'])\n",
        "en_question_Tdata = en_train.map(lambda samples: G_tokenizer.encode_plus(create_prompt2(samples['question']), padding=True), remove_columns=[\"context\", \"answer\", \"question\",'distractors'])[\"input_ids\"]\n",
        "en_question_Vdata = en_validation.map(lambda samples: G_tokenizer.encode_plus(create_prompt2(samples['question']), padding=True), remove_columns=[\"context\", \"answer\", \"question\",'distractors'])[\"input_ids\"]\n",
        "en_train_data=en_train_data.add_column(\"labels\", en_question_Tdata)\n",
        "en_validation_data=en_validation_data.add_column(\"labels\", en_question_Vdata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16c9fd76",
      "metadata": {
        "id": "16c9fd76",
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, EarlyStoppingCallback, Seq2SeqTrainer\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "      gradient_accumulation_steps=10,\n",
        "      per_device_train_batch_size=45,\n",
        "      per_device_eval_batch_size=45,\n",
        "      # save_steps=2,\n",
        "      eval_steps=150,\n",
        "      warmup_steps=150,\n",
        "      logging_steps=150,\n",
        "      weight_decay=0.05,\n",
        "      # save_total_limit=5,\n",
        "      learning_rate=3e-3,\n",
        "      max_steps=3000,\n",
        "      # num_train_epochs=2,\n",
        "      # load_best_model_at_end=True,\n",
        "      # gradient_checkpointing=True,\n",
        "      lr_scheduler_type=\"linear\",\n",
        "      do_train=True,\n",
        "      do_eval=True,\n",
        "      # fp16=False,\n",
        "      report_to=\"all\",\n",
        "      log_level=\"debug\",\n",
        "      logging_dir='./logs',\n",
        "      output_dir='./outputs',\n",
        "      label_names=[\"labels\"],\n",
        "      evaluation_strategy=\"steps\",\n",
        "      # metric_for_best_model=\"eval_loss\",\n",
        "    )\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=L_model,\n",
        "    args=training_args,\n",
        "    tokenizer=G_tokenizer,\n",
        "    train_dataset=en_train_data,\n",
        "    eval_dataset=en_validation_data,\n",
        "    # callbacks=[EarlyStoppingCallback(2, 1.0)],\n",
        "    data_collator=DataCollatorForSeq2Seq(G_tokenizer,label_pad_token_id=-100),\n",
        ")\n",
        "\n",
        "# Additional configuration\n",
        "L_model.config.use_cache = False\n",
        "torch.cuda.empty_cache()\n",
        "# L_model.config.bnb_8bit_compute_type = \"torch.float16\"\n",
        "\n",
        "# Start training\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d90c56e",
      "metadata": {
        "id": "1d90c56e",
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# to hugging face\n",
        "model_name = \"QuestionGeneration\"\n",
        "HUGGING_FACE_USER_NAME = \"mou3az\"\n",
        "\n",
        "L_model.push_to_hub(f\"{HUGGING_FACE_USER_NAME}/{model_name}\", token='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iv_I98LeRLEN",
      "metadata": {
        "id": "iv_I98LeRLEN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Save model checkpoint\n",
        "L_model.save_pretrained(\"QuestionGeneration\")\n",
        "# Create a zip archive\n",
        "!zip -r saved_model.zip QuestionGeneration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fXCtjA79RLEO",
      "metadata": {
        "id": "fXCtjA79RLEO",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#general question generation model\n",
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "HUGGING_FACE_USER_NAME='QuestionGeneration'\n",
        "model_name='mou3az'\n",
        "peft_model_id = f\"{HUGGING_FACE_USER_NAME}/{model_name}\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=False, device_map='auto')\n",
        "G_tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the Lora model\n",
        "L_model = PeftModel.from_pretrained(model, peft_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z2V8EyKzRLEP",
      "metadata": {
        "id": "Z2V8EyKzRLEP",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def generate_questions(context, answer):\n",
        "    device = next(L_model.parameters()).device\n",
        "    input_text = f\"Given the context '{context}' and the answer '{answer}', what question can be asked?\"\n",
        "    encoding = G_tokenizer.encode_plus(input_text, padding=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    output_tokens = L_model.generate(\n",
        "        **encoding,\n",
        "        early_stopping=True,\n",
        "        do_sample= True,\n",
        "        num_beams=5,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,\n",
        "        max_length=256,\n",
        "        temperature=0.6,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.2\n",
        "    )\n",
        "    question = G_tokenizer.decode(output_tokens[0], skip_special_tokens=True).replace(\"question :\", \"\").strip()\n",
        "    return question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_khWtHr9RLEQ",
      "metadata": {
        "id": "_khWtHr9RLEQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from rouge import Rouge\n",
        "from fuzzywuzzy import fuzz\n",
        "from bert_score import score\n",
        "\n",
        "\n",
        "def calculate_bleu_scores(references, predictions):\n",
        "    return corpus_bleu([[ref.split()] for ref in references], [pred.split() for pred in predictions])\n",
        "\n",
        "def calculate_rouge_scores(references, predictions):\n",
        "    rouge = Rouge()\n",
        "    rouge_scores = rouge.get_scores(predictions, references, avg=True)\n",
        "    return rouge_scores\n",
        "\n",
        "def calculate_accuracy(references, predictions):\n",
        "    accuracies = [fuzz.token_sort_ratio(ref, pred) / 100.0 for ref, pred in zip(references, predictions)]\n",
        "    return sum(accuracies) / len(accuracies)\n",
        "\n",
        "def calculate_bert_score(references, predictions):\n",
        "    P, R, F1 = score(predictions, references, lang='en', verbose=False)\n",
        "    return F1.mean().item()\n",
        "\n",
        "def evaluate(dataset):\n",
        "    references = [sample['question'] for sample in dataset]\n",
        "    predictions = [generate_questions(sample['context'], sample['answer']) for sample in dataset]  # Assuming 'generate_questions' generates model's output\n",
        "\n",
        "    bleu_score = calculate_bleu_scores(references, predictions)\n",
        "    rouge_scores = calculate_rouge_scores(references, predictions)\n",
        "    accuracy = calculate_accuracy(references, predictions)\n",
        "    bert_score = calculate_bert_score(references, predictions)\n",
        "\n",
        "    print(\"Overall Accuracy:\", accuracy)\n",
        "    print(\"Overall BLEU Score:\", bleu_score)\n",
        "    print(\"Overall ROUGE Score:\", rouge_scores)\n",
        "    print(\"Overall BERTScore:\", bert_score)\n",
        "\n",
        "    return accuracy, bleu_score, rouge_scores, bert_score\n",
        "\n",
        "# Assuming 'en_validation' is your dataset\n",
        "accuracy, bleu_score, rouge_scores, bert_score = evaluate(en_validation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cLu71ILVRb9_",
      "metadata": {
        "id": "cLu71ILVRb9_"
      },
      "source": [
        "# ###########################################"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "pYfKMpAjRLDr",
        "7b3f1d57",
        "6d83352b",
        "05818b09",
        "z8pxtjdmRLEA",
        "h0qGw8h7RSqE"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 4530578,
          "sourceId": 7749513,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4545907,
          "sourceId": 7770645,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4546679,
          "sourceId": 7771818,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4561912,
          "sourceId": 7792920,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4564047,
          "sourceId": 7795842,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4564227,
          "sourceId": 7796098,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30665,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
